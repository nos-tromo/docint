services:
  ##########################################################
  # Backend (CPU)
  ##########################################################
  backend:
    profiles: ["cpu"]
    build:
      context: .
      dockerfile: Dockerfile.backend.cpu
    ports:
      - "8000:8000"
    environment:
      - DATA_PATH=/app/data
      - LOG_PATH=/app/.logs/backend-cpu.log
      - OPENAI_API_BASE=http://llamacpp-server:8080/v1
      - QDRANT_COL_DIR=/qdrant/storage/collections
      - QDRANT_HOST=http://qdrant:6333
      - QDRANT_SRC_DIR=/qdrant/storage/sources
      - QUERIES_PATH=/app/data/queries.txt
      - RESULTS_PATH=/app/results
      - USE_DEVICE=auto
      - LLAMA_CPP_N_GPU_LAYERS=0
    env_file:
      - .env.docker
    volumes:
      - ${USERPROFILE:-$HOME}/docint/data:/app/data
      - ${USERPROFILE:-$HOME}/docint/results:/app/results
      - docint-logs:/app/.logs
      - model-cache:/root/.cache
      - qdrant-storage:/qdrant/storage
    networks:
      docint-net:
        aliases:
          - backend-net

  ##########################################################
  # Backend (GPU)
  ##########################################################
  backend-gpu:
    profiles: ["gpu"]
    build:
      context: .
      dockerfile: Dockerfile.backend.cuda
    ports:
      - "8000:8000"
    environment:
      - DATA_PATH=/app/data/in
      - LOG_PATH=/app/.logs/backend-gpu.log
      - NVIDIA_VISIBLE_DEVICES=all
      - OPENAI_API_BASE=http://llamacpp-server:8080/v1
      - QDRANT_COL_DIR=/qdrant/storage/collections
      - QDRANT_HOST=http://qdrant:6333
      - QDRANT_SRC_DIR=/qdrant/storage/sources
      - QUERIES_PATH=/app/data/queries.txt
      - RESULTS_PATH=/app/data/out
      - USE_DEVICE=auto
      - LLAMA_CPP_N_GPU_LAYERS=-1
    env_file:
      - .env.docker
    volumes:
      - ${USERPROFILE:-$HOME}/docint/data:/app/data
      - ${USERPROFILE:-$HOME}/docint/results:/app/results
      - docint-logs:/app/.logs
      - model-cache:/root/.cache
      - qdrant-storage:/qdrant/storage
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    networks:
      docint-net:
        aliases:
          - backend-net

  ##########################################################
  # Llama.cpp (CPU)
  ##########################################################
  llamacpp-server-cpu:
    profiles: ["cpu"]
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - 8080:8080
    # Ensure the models directory exists before starting the server to prevent crash
    entrypoint: ["/bin/sh", "-c", "mkdir -p /root/.cache/llama.cpp && /app/llama-server"]
    environment:
      LLAMA_ARG_EMBEDDINGS: 1
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_HOST: 0.0.0.0
      LLAMA_ARG_MODELS_DIR: /root/.cache/llama.cpp
      LLAMA_ARG_PORT: 8080
      LLAMA_ARG_UBATCH: 1024
      LLAMA_ARG_ENDPOINT_PROPS: 1
      LLAMA_LOG_FILE: /app/.logs/llamacpp-server-cpu.log
      LLAMA_OFFLINE: true
    volumes:
      - docint-logs:/app/.logs
      - model-cache:/root/.cache
    networks:
      docint-net:
        aliases:
          - llamacpp-server
    restart: unless-stopped

  ##########################################################
  # Llama.cpp (CUDA)
  ##########################################################
  llamacpp-server-cuda:
    profiles: ["cuda"]
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    ports:
      - 8080:8080
    entrypoint: ["/bin/sh", "-c", "mkdir -p /root/.cache/llama.cpp"]
    environment:
      LLAMA_ARG_EMBEDDINGS: 1
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_HOST: 0.0.0.0
      LLAMA_ARG_MODELS_DIR: /root/.cache/llama.cpp
      LLAMA_ARG_PORT: 8080
      LLAMA_ARG_UBATCH: 1024
      LLAMA_LOG_FILE: /app/.logs/llamacpp-server-cuda.log
      LLAMA_OFFLINE: true
      NVIDIA_VISIBLE_DEVICES: all
    volumes:
      - model-cache:/root/.cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [cuda]
    networks:
      docint-net:
        aliases:
          - llamacpp-server
    restart: unless-stopped

  ##########################################################
  # Frontend (Streamlit)
  ##########################################################
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    command: streamlit run docint/app.py --server.address 0.0.0.0
    ports:
      - "8501:8501"
    environment:
      - BACKEND_HOST=http://backend-net:8000
      - BACKEND_PUBLIC_HOST=http://localhost:8000
      - LOG_PATH=/app/.logs/frontend.log
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
    volumes:
      - ${USERPROFILE:-$HOME}/docint/data:/app/data
      - ${USERPROFILE:-$HOME}/docint/results:/app/results
      - docint-logs:/app/.logs
    networks:
      - docint-net

  ##########################################################
  # Qdrant
  ##########################################################
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    environment:
      - QDRANT__TELEMETRY_DISABLED=true
    volumes:
      - qdrant-snapshots:/qdrant/snapshots
      - qdrant-storage:/qdrant/storage
    restart: unless-stopped
    networks:
      - docint-net

##########################################################
# Volumes
##########################################################
volumes:
  docint-logs:
  model-cache:
  qdrant-snapshots:
  qdrant-storage:

##########################################################
# Networks
##########################################################
networks:
  docint-net:
