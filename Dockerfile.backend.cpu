FROM python:3.11-slim-bookworm

ENV DEBIAN_FRONTEND=noninteractive \
    UV_CACHE_DIR=/root/.cache/uv \
    VIRTUAL_ENV=/app/.venv \
    PATH="/app/.venv/bin:$PATH" \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONIOENCODING=utf-8 \
    PYTHONUNBUFFERED=1

WORKDIR /app

RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential ca-certificates libmagic1 libgl1 cmake libopenblas-dev \
 && rm -rf /var/lib/apt/lists/*

COPY --from=ghcr.io/astral-sh/uv:0.9.28 /uv /uvx /bin/

RUN uv venv

# Copy dependency files
# We only use pyproject.toml for CPU build because uv.lock resolves to CUDA on Linux
COPY pyproject.toml ./

# Install dependencies (using cache mount to speed up downloads)
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install \
    --index-url https://download.pytorch.org/whl/cpu \
    --extra-index-url https://pypi.org/simple \
    -r pyproject.toml

# Install llama-cpp-python for CPU
RUN --mount=type=cache,target=/root/.cache/uv \
    CMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS" \
    uv pip install llama-cpp-python --no-cache-dir

COPY . .

RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --no-deps .

EXPOSE 8000

CMD ["sh", "-c", "load-models && uvicorn docint.core.api:app --host 0.0.0.0 --port 8000"]
