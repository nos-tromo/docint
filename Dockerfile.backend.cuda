FROM nvidia/cuda:13.0.2-cudnn-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive \
    UV_CACHE_DIR=/root/.cache/uv \
    PATH="/app/.venv/bin:$PATH" \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONIOENCODING=utf-8 \
    PYTHONUNBUFFERED=1

WORKDIR /app

RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential ca-certificates libmagic1 libgl1 cmake zlib1g-dev \
 && rm -rf /var/lib/apt/lists/*

COPY --from=ghcr.io/astral-sh/uv:0.9.28 /uv /uvx /bin/

# Copy dependency files first to leverage Docker layer caching
COPY pyproject.toml uv.lock ./

# Install dependencies (without the project itself) using cache mount to speed up downloads
RUN --mount=type=cache,target=/root/.cache/uv \
    uv sync --frozen --no-dev --no-install-project && \
    uv pip uninstall fastembed onnxruntime && \
    uv pip install fastembed-gpu onnxruntime-gpu && \
    CMAKE_ARGS="-DGGML_CUDA=on" \
    uv pip install llama-cpp-python --no-cache-dir --force-reinstall --upgrade

COPY . .

# Install the project package
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --no-deps .

EXPOSE 8000

CMD ["sh", "-c", "uv run load-models && uv run uvicorn docint.core.api:app --host 0.0.0.0 --port 8000"]
