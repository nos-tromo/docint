DOCINT_OFFLINE=true
ENABLE_IE=true
IE_MAX_WORKERS=4
# PRELOAD_MODELS=false
# Model Configuration
LLM=unsloth/Qwen3-1.7B-GGUF
LLM_FILE=Qwen3-1.7B-Q4_K_M.gguf
LLM_TOKENIZER=Qwen/Qwen3-1.7B
# Llama.cpp Configuration (Uncomment to override defaults)
# LLAMA_CPP_N_GPU_LAYERS=-1  # -1 = offload all layers to GPU, 0 = CPU only
# LLAMA_CPP_CTX_WINDOW=32768
# LLAMA_CPP_MAX_NEW_TOKENS=1024  # Maximum tokens per generation
# LLAMA_CPP_TEMPERATURE=0.1