DOCINT_OFFLINE=true
ENABLE_IE=true
IE_MAX_WORKERS=4
# PRELOAD_MODELS=false

# --- Inference backend ---
# Choose one profile when starting:
#   docker compose --profile cpu-llamacpp up   (default)
#   docker compose --profile cpu-ollama up
#   docker compose --profile cpu-openai up
#   docker compose --profile cuda-llamacpp up
#   docker compose --profile cuda-ollama up
#   docker compose --profile cuda-openai up

# --- OpenAI-compatible API settings ---
OPENAI_API_KEY=sk-no-key-required
# Timeout (seconds) for API requests. CPU inference may need a higher
# value (default: 300).
# OPENAI_TIMEOUT=300

# --- Llama.cpp (default) ---
# No extra settings needed; the profile sets OPENAI_API_BASE and
# INFERENCE_SERVER automatically.

# --- Ollama ---
# Use a cpu-ollama / cuda-ollama profile. OPENAI_API_BASE and
# INFERENCE_SERVER are set automatically by the profile.
# EMBED_MODEL=bge-m3
# LLM=gpt-oss:20b
# VLM=qwen3-vl:8b

# --- External OpenAI API ---
# Use a cpu-openai / cuda-openai profile and set the variables below.
# OPENAI_API_KEY=sk-your-key-here
# OPENAI_API_BASE=https://api.openai.com/v1
